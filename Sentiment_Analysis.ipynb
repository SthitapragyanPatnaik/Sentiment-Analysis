{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis A-team.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AMJXEY9r4Af6",
        "t1UeI5_uL6sn",
        "C-Bu4eny4UPu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMJXEY9r4Af6"
      },
      "source": [
        "# General Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SD5VWS44E-Q"
      },
      "source": [
        "#When working on jupyter/spyder notebook on your local machine, run the commands for downloading the below libraries before importing\n",
        "\n",
        "# pip install -U spacy\n",
        "# pip install -U spacy-lookups-data\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download en_core_web_md\n",
        "# python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbaRRO7u4GOn"
      },
      "source": [
        "#Importing the required libraries\n",
        "\n",
        "!pip install wordcloud\n",
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler \n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUipdyM74GU_"
      },
      "source": [
        "#Importing the given training data \n",
        "input_train=pd.read_csv('train.csv')\n",
        "input_train.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6NzQlFTYpR7"
      },
      "source": [
        "#Importing the given test data file\n",
        "df_test = pd.read_csv('test_samples.csv')\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuSSHR5A6lfI"
      },
      "source": [
        "#Checking value counts of each sentiment in the input training data\n",
        "input_train['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1UeI5_uL6sn"
      },
      "source": [
        "# Processing the input tweets and Extracting Meaningful data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfl1IqiR6lXZ"
      },
      "source": [
        "#Counting Word Counts of each tweet in a row \n",
        "input_train['word_counts']=input_train['tweet_text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "#Counting Characters in each tweet in a row \n",
        "input_train['char_counts']=input_train['tweet_text'].apply(lambda x: len(x))\n",
        "\n",
        "# Average Word Length in each tweet\n",
        "def get_avg_word_len(x):\n",
        "    words=x.split()\n",
        "    word_len=0\n",
        "    for word in words:\n",
        "        word_len=word_len +len(word)\n",
        "    return word_len/len(words)\n",
        "\n",
        "input_train['avg_word_len']=input_train['tweet_text'].apply(lambda x: get_avg_word_len(x))\n",
        "\n",
        "#Removing stop words from each tweet\n",
        "input_train['stop_words_len'] =input_train['tweet_text'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
        "\n",
        "#Count #HashTags and @Mentions in each tweet\n",
        "input_train['hashtags_count'] = input_train['tweet_text'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
        "input_train['mentions_ count'] = input_train['tweet_text'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
        "\n",
        "###if numeric digit is present in tweet\n",
        "input_train['numeric_count']=input_train['tweet_text'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
        "\n",
        "#Counting upper case words in each tweet\n",
        "input_train['upper_counts'] = input_train['tweet_text'].apply(lambda x: len([t for t in x.split() if t.isupper()]))\n",
        "\n",
        "#Checking the first five rows of dataframe with the added new columns\n",
        "input_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXWZOGocY5-"
      },
      "source": [
        "#Converting all the tweets into lowercase\n",
        "input_train['tweet_text'] = input_train['tweet_text'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJPkgp3dKCJ"
      },
      "source": [
        "#dictionay of short forms and its replacement with the proper word\n",
        "\n",
        "contractions = {\n",
        "\"ain't\": \"am not\",    \n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't have\": \"cannot have\",\n",
        "\"'cause\":\"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I will\",\n",
        "\"i'll've\": \"I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"musn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \"you\",\n",
        "\" ur \": \"your\",\n",
        "\" n \": \"and\",\n",
        "}\n",
        "\n",
        "def cont_to_exp(x):\n",
        "  if type(x) is str:\n",
        "    for key in contractions:\n",
        "      value = contractions[key]\n",
        "      x = x.replace(key, value)\n",
        "    return x\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIc1mO18lEX9"
      },
      "source": [
        "#Removing any email IDs present in the tweets\n",
        "input_train['tweet_text'] = input_train['tweet_text'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9+._-]+\\.[a-zA-Z0-9+._-]+)',\"\",x))\n",
        "\n",
        "#Removing any URLs present in the tweets\n",
        "input_train['tweet_text'] = input_train['tweet_text'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\"\", x)) \n",
        "\n",
        "#Removing tag \"RT\" - retweet present in the tweets\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: re.sub('RT',\"\",x))\n",
        "\n",
        "#Removing any other special character present in the tweets\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
        "\n",
        "#Removing digits present in the tweets\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: re.sub('[0-9]', '', x))\n",
        "\n",
        "#Checking the first five rows of the dataframe after the above changes\n",
        "input_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSA38QPApYLA"
      },
      "source": [
        "#Each row of tweet_text will be converted back to string from list of words\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: \" \".join(x.split()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZLrJs2Pp4vo"
      },
      "source": [
        "#Removing the HTML tags present in the tweets\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
        "\n",
        "#Removing the Stop words from each tweet\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpDuDfvNrqFG"
      },
      "source": [
        "#5 mins\n",
        "#Downloading spacy en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Function for applying lemmatization in each tweet - base form of the word\n",
        "def make_to_base(x):\n",
        "  x_list = []\n",
        "  doc = nlp(x)\n",
        "\n",
        "  for token in doc:\n",
        "    lemma = str(token.lemma_)\n",
        "    if lemma == '-PRON-' or lemma == 'be':\n",
        "      lemma = token.text\n",
        "    x_list.append(lemma)\n",
        "\n",
        "  return (\" \".join(x_list))\n",
        "\n",
        "#Calling function for each row of the dataframe\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: make_to_base(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQ_p3wIsWlO"
      },
      "source": [
        "#Removing the most common and most rare 20 words \n",
        "\n",
        "text = ' '.join(input_train['tweet_text'])\n",
        "text = text.split()\n",
        "freq_comm = pd.Series(text).value_counts()\n",
        "f20 = freq_comm[:20] \n",
        "rare20 = freq_comm[-20:] \n",
        "\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in f20]))\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in rare20]))\n",
        "\n",
        "input_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ypkffPxwza4"
      },
      "source": [
        "#Word Cloud for the most common words\n",
        "x = ' '.join(text)\n",
        "wc = WordCloud(width = 800, height = 400).generate(x)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te842BEYyDni"
      },
      "source": [
        "#Tokenization\n",
        "input_train['tweet_text']=input_train['tweet_text'].apply(lambda x: TextBlob(str(x)).words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Bu4eny4UPu"
      },
      "source": [
        "# ML models for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMRcrSa-4XqM"
      },
      "source": [
        "df0=input_train[input_train['sentiment']=='positive']\n",
        "df2=input_train[input_train['sentiment']=='neutral']\n",
        "df4=input_train[input_train['sentiment']=='negative']\n",
        "dfr=df0.append(df4)\n",
        "dfr=dfr.append(df2)\n",
        "\n",
        "dfr['tweet_text'] = dfr['tweet_text'].apply(lambda x: \" \".join(x)) \n",
        "dfr_feat=dfr.drop(labels=['tweet_text', 'sentiment'], axis=1)\n",
        "y=dfr['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyEpES68xfx"
      },
      "source": [
        "sgd=SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\n",
        "lgr=LogisticRegression(random_state=42, max_iter=200)\n",
        "lgrcv=LogisticRegressionCV(cv=2, random_state=42, max_iter=1000)\n",
        "svm=LinearSVC(random_state=42, max_iter=200)\n",
        "rfc=RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)\n",
        "\n",
        "clf={'SGD': sgd, 'LR': lgr, 'LGR-CV': lgrcv, 'SVM': svm, 'RFC':rfc}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tujk4zUaBiAF"
      },
      "source": [
        "def classify(X, y):\n",
        "  scaler=MaxAbsScaler()\n",
        "  X=scaler.fit_transform(X)\n",
        "\n",
        "  X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "  for key in clf.keys():\n",
        "    clf[key].fit(X_train, y_train)\n",
        "\n",
        "    y_pred=clf[key].predict(X_test)\n",
        "    ac=accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzUwUHQsHKpJ"
      },
      "source": [
        "%%time\n",
        "classify(dfr_feat, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiWkVJ1xH99_"
      },
      "source": [
        "#7 mins\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X=tfidf.fit_transform(dfr['tweet_text'])\n",
        "\n",
        "scaler=MaxAbsScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "for key in clf.keys():\n",
        "  clf[key].fit(X_train, y_train)\n",
        "\n",
        "  y_pred=clf[key].predict(X_test)\n",
        "  ac=accuracy_score(y_test, y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3vndJTh7uuW"
      },
      "source": [
        "# Repeating the same steps for cleaning and processing the test data as done for training data\n",
        "#10 mins\n",
        "\n",
        "\n",
        "df_test['word_counts']=df_test['tweet_text'].apply(lambda x: len(str(x).split()))\n",
        "df_test['char_counts']=df_test['tweet_text'].apply(lambda x: len(x))\n",
        "df_test['avg_word_len']=df_test['tweet_text'].apply(lambda x: get_avg_word_len(x))\n",
        "df_test['stop_words_len'] =df_test['tweet_text'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
        "df_test['hashtags_count'] = df_test['tweet_text'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
        "df_test['mentions_ count'] = df_test['tweet_text'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
        "df_test['numeric_count']=df_test['tweet_text'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
        "df_test['upper_counts'] = df_test['tweet_text'].apply(lambda x: len([t for t in x.split() if t.isupper()]))\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(lambda x: x.lower())\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9+._-]+\\.[a-zA-Z0-9+._-]+)',\"\",x))\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\"\", x))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: re.sub('RT',\"\",x))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: re.sub('[0-9]', '', x))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: \" \".join(x.split()))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: make_to_base(x))\n",
        "text_test = ' '.join(df_test['tweet_text'])\n",
        "text_test = text_test.split()\n",
        "freq_comm = pd.Series(text_test).value_counts()\n",
        "f20 = freq_comm[:20] #\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in f20]))\n",
        "rare20 = freq_comm[-20:]\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in rare20]))\n",
        "x = ' '.join(text_test)\n",
        "df_test['tweet_text']=df_test['tweet_text'].apply(lambda x: TextBlob(str(x)).words)\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(lambda x: \" \".join(x)) \n",
        "dfr_feat=df_test.drop(labels=['tweet_text'], axis=1)\n",
        "\n",
        "\n",
        "X_train = df_test['tweet_text']\n",
        "X_train = tfidf.transform(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "\n",
        "pred_list=[]\n",
        "\n",
        "for key in clf.keys():\n",
        "  print(key)\n",
        "  y_pred=clf[key].predict(X_train)\n",
        "  pred_list.append(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_ZdSjIXzoZ"
      },
      "source": [
        "#downloading the final output files\n",
        "\n",
        "from google.colab import files\n",
        "df_SGD = pd.DataFrame(df_test.iloc[:,0])\n",
        "df_SGD['sentiment']=pred_list[0]\n",
        "df_SGD.to_csv('output_SGD.csv',index=False)\n",
        "# files.download('output_SGD.csv')\n",
        "\n",
        "df_LR = pd.DataFrame(df_test.iloc[:,0])\n",
        "df_LR['sentiment']=pred_list[1]\n",
        "df_LR.to_csv('output_LR.csv',index=False)\n",
        "# files.download('output_LR.csv')\n",
        "\n",
        "df_LGRCV = pd.DataFrame(df_test.iloc[:,0])\n",
        "df_LGRCV['sentiment']=pred_list[2]\n",
        "df_LGRCV.to_csv('output_LGRCV.csv',index=False)\n",
        "# files.download('output_LGRCV.csv')\n",
        "\n",
        "df_SVM = pd.DataFrame(df_test.iloc[:,0])\n",
        "df_SVM['sentiment']=pred_list[3]\n",
        "df_SVM.to_csv('output_SVM.csv',index=False)\n",
        "# files.download('output_SVM.csv')\n",
        "\n",
        "#the model which gives the best accuracy \n",
        "df_RFC = pd.DataFrame(df_test.iloc[:,0])\n",
        "df_RFC['sentiment']=pred_list[4]\n",
        "df_RFC.to_csv('output_RFC.csv',index=False)\n",
        "files.download('output_RFC.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}